<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">

<head>
  <meta charset="utf-8" />
  <title>Horus • Case Study</title>
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto&family=Rubik:wght@500&display=swap" rel="stylesheet"> 
  <script type="text/javascript">
    WebFont.load({ google: { families: ["Inter:regular,500,600,700"] } });
  </script>
  <script type="text/javascript">
    !(function (o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      (n.className += t + "js"),
        ("ontouchstart" in o ||
          (o.DocumentTouch && c instanceof DocumentTouch)) &&
        (n.className += t + "touch");
    })(window, document);
  </script>
  <link href="assets/images/logo-mono.png" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/images/logo-mono.png" rel="apple-touch-icon" />
  <script src="https://kit.fontawesome.com/d019875f94.js" crossorigin="anonymous"></script>
  <meta name="image" property="og:image" content="assets/images/thumbnail.png" />
</head>

<body>
  <div class="navigation-wrap">
    <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
      <div class="navigation-container">
        <div class="navigation-left">
          <a href="/" aria-current="page" class="brand w-nav-brand w—current" aria-label="home">
            <img src="assets/images/horus_logo color.png" alt="" class="template-logo" />
          </a>
        </div>
        <div class="navigation-right">
          <nav role="navigation" class="nav-menu w-nav-menu">
            <a href="/case-study" class="link-block w-inline-block">
              <div>Case Study</div>
            </a>
            <a href="/team" class="link-block w-inline-block">
              <div>Team</div>
            </a>
          </nav>
          <div class="login-buttons">
            <a href="https://github.com/try-horus" target="_blank">
              <span style="color: #081b53">
                <i class="fab fa-github fa-lg"></i>
              </span>
            </a>
          </div>
        </div>
      </div>
      <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
    </div>
  </div>
  <div id="sidebar" class="toc">
  </div>
  <div class="section header">
    <article class="container case-study-container">
      <div class="hero-text-container">
        <h1 class="h1 centered">Case Study</h1>
      </div>
      <div id="case-study">

        <br />
        <br />

        <!-- Section 1 -->
        <h2 class="h2">1. Introduction</h2>
        <br>
        <p>
          Transitioning to a microservice architecture brings many benefits to a growing application. The scalability by component, separation of concerns, and flexibility of technology used are just some of the ways it provides immediate benefits. The main drawback of a microservice architecture is it exponentially increases the age-old problem of any technology: what if something stops working?
        </p>
        <br>
        <p>
          The question is not if something will fail, but when. What makes this worse in a microservice vs. monolith architecture is that finding the source of failure is much harder. With the increased complexity created by each additional service, every relationship and dependency, no matter how small, could be the culprit. Was it the checkout service? Inventory? Authentication? The thank-you email?
        </p>
        <br>
        <p>
          Knowing which service failed is only the first step — next is knowing why it failed. Was it due to a spike in traffic? Was it a bug in the code? Bad input from the user? 
        </p>
        <br>
        <p>
          Knowing and understanding the source of the problem is crucial. This is where telemetry data, specifically metrics and tracing, come in. With metrics, the user can see the number of requests and errors hitting their application for a given interval. With distributed tracing, the user can track how a request travels through their microservice architecture, from the moment a user clicks a button until a response is sent back to them. 
        </p>
        <br>
        <p>
          Metrics are incredibly useful to alert that there’s a problem (e.g. errors go up in metrics), and traces provide that next step to find where it took place (e.g. there was a 500 error when the user submitted their email address). The correlation between the metrics and traces makes the debugging process a lot easier, especially with multiple services working together.
        </p>
        <br>
        <p>
          Let's look at a concrete example of where this need often arises: Evo T-shirts.
        </p>

        <h3>1.1 Evo T-Shirts</h3>
        <p>
          Evo T-shirts, an e-commerce site, has recently skyrocketed in popularity. What started as one person’s side-hustle has now outgrown a monolithic architecture. The powers that be have decided to move to a microservice architecture for better organization. Oliver has led the team of engineers who built Evo T-shirts for years, and he continues to oversee each sub-team in their now assigned microservice. 
        </p>
        <figure>
          <img src="assets/images/case-study/no-errors.gif" class="case-study-image" />
          <figcaption>Part of Evo T-Shirt's architecture</figcaption>
        </figure>
        <br />
        <p>
          One day, he wakes up and finds that the site has been inundated by a stream of bad reviews overnight. What happened? Customers are complaining about not being able to buy products, and many of them sign off swearing they’ll never try the site again. While Oliver is confused, the CEO wants answers. Now.
        </p>
        <br />
        <p>
          Oliver quickly jumps on the site and buys some t-shirts, trying to recreate the errors that customers are experiencing. Unfortunately, he doesn't get any errors, his cart is accurate, and he is able to complete his purchase without incident. He checks the status of each microservice, but they’re all running just fine. What is going on?
        </p>
        <br />
        <p>
          Several engineers on his team try recreating the problem, too. Some of them receive the same errors customers were complaining about, while others do not. They’re not sure why it’s happening for some and not others. Oliver’s last resort is to look at the logs of each microservice, hoping they’ll contain information about the alleged errors. 
        </p>
        <figure>
          <img src="assets/images/case-study/endless-logs.jpg" class="case-study-image" />
        </figure>
        <br />
        <p>
          His frustration mounts as he continues to scroll. The logs are endless and difficult for him to parse. He sees something about an error here and there, but it’s unclear where it’s happening or why.
        </p>
        <br>
        <p>
          Oliver and his team would greatly benefit from an observability solution right about now. Horus would provide just that. 
        </p>

        <h3>1.2 What is Horus?</h3>
        <p>
          Horus is an open-source observability solution for microservices. Users can generate, store, and visualize correlated telemetry data, allowing them to see the real-time health of their application. 
        </p>
        <br />
        <p>
          Because the metrics and traces are correlated, the user can click on a particular metric data point and view all the traces that happened in that time interval. This allows the user to easily discover where the customer of the app received an error, which makes it easier to start fixing it.
        </p>
        <br />
        <p>
          If there is an uptick in errors, Oliver can quickly jump through that metric error data point and see traces that happened in that timeframe. By deep-diving into the traces, he can see the exact request and exact error. Then all he has to do is fix it. 
        </p>
        <br />
        <p>
          Before diving further into Horus, let’s take a step back and better understand the realm of telemetry data and observability. 
        </p>
        <br />
        
        <!-- Section 2 -->
        <h2 class="h2">2 Telemetry</h2>
        <h3>2.1 What is telemetry?</h3>
        <p>
          Telemetry is “the collection of measurements, or other data, at remote or inaccessible points and their automatic transmission to receiving equipment for monitoring." Put another way, telemetry is a collection of data points that represent information about an application's health and can be used for monitoring and observability purposes. Want to know how your system is doing? Look at the telemetry data.
        </p>
        <br />
        <p>
          There are at least three essential observability data types: <span class="bold">metrics</span>, <span class="bold">logs</span>, and <span class="bold">traces</span>.
        </p>
        <br />
        <h3>2.2 What are metrics, traces, and logs?</h3>
        <p>
          <span class="bold">Metrics</span> are numeric measurements. A metric could represent a numeric status for a moment in time (% of memory being used) or it could be an aggregated measurement (how many clicks per 10 seconds). A metric data point usually includes a name, a timestamp, and one or more numeric values. Metrics are considered to be the macro level of telemetry data and can answer questions like:
          <ul>
            <li>“Is the application performing slower than it should be?”</li>
            <li>“Are users encountering errors while using the application?</li>
            <li>"At what time of day are we receiving the most amount of requests?"</li>
          </ul>
        </p>
        <br />
        <p>
          Why the application is performing this way may still be unclear, but the metric brings clarity to the broader picture of the application's overall state and performance. For example, when monitoring the app’s network, there could be a sharp uptick in requests per second. This could indicate good business — or a DDoS attack. Either way, the information encourages the developer to pay attention and learn more.
        </p>
        <figure>
          <img src="assets/images/case-study/metric-example.png" class="case-study-image" />
          <figcaption>A metric graph from DataDog</figcaption>
        </figure>
        <br />
        <p>
          <span class="bold">Logs</span> are the most detailed data type, so it’s easy to assume they’re immediately helpful. They can be, but all of the details can make them hard to parse and understand. Logs are often unstructured, sporadic lines of text that the system creates as it does various tasks. Logs are great when diving into the details and seeing the step-by-step explanation of what a system did, or attempted to do.
        </p>
        <figure>
          <img src="assets/images/case-study/log-example.png" class="case-study-image" />
        </figure>
        <br />
        <p>
          <span class="bold">Traces</span> are the newest telemetry data type and are used less than metrics and logs. As a newer technology, it can sometimes be harder to work with them in a meaningful way. However, when properly captured and queried, they offer a wealth of information.
        </p>
        <br />
        <p>
          A request can pass through a host of different services, making them harder to pin down and understand what happened. A trace provides a view of a request throughout its lifetime in a system: where it starts, where it goes to next, and where it finally terminates, throughout a distributed infrastructure.
        </p>
        <br />
        <p>
          Traces capture not only the whole journey of request, but each step of the way, and how long each step took. Each step is commonly referred to as a span (short for a span of time).
        </p>
        <figure>
          <img src="assets/images/case-study/trace-example.png" class="case-study-image" />
          <figcaption>A distributed trace from DataDog</figcaption>
        </figure>
        <br />

        <h3>2.3 Metrics and Traces for Evo T-Shirts</h3>
        <p>
          With that context in mind, let's return to Evo T-shirts. Aside from his immediate need, Oliver and his team generally want to be able to get a bird's-eye view of how Evo T-shirts is performing over a daily or weekly period. If the marketing team decides to run a promotion or sale, they may even want to visualize how the online store is performing on an hourly basis. Any dropped requests, errors encountered, or steep increases in latency could impact the overall user experience, costing the company money.
        </p>
        <br />
        <p>
          Metrics seem like a good path to pursue here. Oliver could simply run a monitoring system alongside the application, see the overall health of the store, and get instant feedback on any issues that occur. 
        </p>
        <br>
        <p>
          Metrics aggregate all monitoring measurements and export them at specific time intervals. All metric data is included in this aggregation and the exporting cost does not increase with higher traffic. Therefore, a metric measuring the average latency of an endpoint will also include the fastest and slowest requests that took place in that time period. A calculation of the error rate will always be accurate as the counter will only be incremented if an error is thrown in the application. 
        </p>
        <br>
        <p>
          Although this is a great place to start, there are some limitations in taking a metric-only approach.
        </p>
        <br>
        <p>
          If Oliver's application were to start detecting errors, or there was a sharp increase in overall latency, Oliver wouldn't be able to identify where across his microservice infrastructure the complications were materializing. Oliver needs to meaningfully query the telemetry data, using the metrics as indicators, to resolve the issue and get the application back up and running again. 
        </p>
        <br>
        <p>
          Traces are the natural option here. They provide detailed breakdowns of individual requests by supplying contextual information around how the journey of the request unfolded as it moved through the system's infrastructure. Furthermore, traces excel at showing the relationship between services as well as capturing high-cardinality data about each specific request. This helps pinpoint the source of an issue in the distributed system quickly and accurately.
        </p>
        <br>
        <p>
          By establishing a relationship between the macro nature of metrics and detailed nuances of traces, Oliver is able to achieve the observability coverage he's looking for by recording the interactions that are taking place across the distributed services. Oliver can then "jump" between a high-level and low-level perspective by switching between metrics and traces. 
        </p>
        <figure>
          <img src="assets/images/case-study/evo-with-horus.gif" class="case-study-image" />
          <figcaption>Evo T-Shirts with Horus instrumentation via horus agent</figcaption>
        </figure>
        <br>
        <p>
          All of this looks like an exciting path forward to solving the application monitoring problem for Oliver and Evo T-shirts, but how does a telemetry system work exactly? Let's look at the components and architecture that make this monitoring solution possible.
        </p>
        <br>
        <h3>2.4 A Telemetry System Architecture</h3>
        <p>
          In <i>Software Telemetry</i>, Jamie Riedesel describes software telemetry as "the systems that bring you telemetry and display it in a way that will help you make decisions."
        </p>
        <br>
        <p>
          A software telemetry system is composed of three major stages: <span class="bold">emission</span>, <span class="bold">shipping</span>, and <span class="bold">presentation</span>. In essence, it will ingest data, process it, store it, and make it retrievable to the end-user. This applies to every type of telemetry data: <span class="bold">metrics</span>, <span class="bold">traces</span>, and <span class="bold">logs</span>. Let’s break down these stages and see what function they play in a wider telemetry system.
        </p>
        <figure>
          <img src="assets/images/case-study/telemetry-pipeline.png" class="case-study-image" />
          <figcaption>The Telemetry Pipeline</figcaption>
        </figure>
        <br>
        <h4>The Emitting Stage</h4>
        <p>
          The emitting stage is the first stage in the telemetry pipeline architecture. It is largely responsible for generating telemetry data in a production application. This stage can either allow the telemetry data to be emitted and stored locally, or it can be forwarded to the shipping stage to be batched, processed, and exported to a remote location. 
        </p>
        <br>
        <p>
          For Oliver, it's important that his own application is decoupled from the telemetry system so that both components are able to operate independently of one another. It's common practice today to implement a telemetry generating emitter through the form of an SDK. OpenTelemetry is an example of a leading developer tool kit that could be implemented to generate and emit telemetry data. We'll discuss OpenTelemetry in greater depth later on.
        </p>
        <figure>
          <img src="assets/images/case-study/emitting.png" class="case-study-image case-study-image-smaller" />
        </figure>
        <br>
        <h4>The Shipping Stage</h4>
        <p>
          The shipping stage is largely responsible for receiving the raw telemetry data from the emitter (in the form of either metrics, logs, or traces), processing it into an accessible data format, enriching the data if needed, and saving the data in a storage mechanism. A large design component of the shipping stage is to consider how to organize and store the raw telemetry data that is generated by your emitter. 
        </p>
        <br>
        <p>
          Telemetry data is generated continuously and at a high frequency. Therefore, the database needs to be able to handle this high write intensity while also being able to query data when required. Should the frequency of the incoming data surpass the processing capability of the database, it may be wise to consider the implementation of either a queue or event bus to manage the data influx. Both these options also provide the benefit of reducing the possibility of data being dropped if there is a database backlog or being lost if the database goes down.
        </p>
        <figure>
          <img src="assets/images/case-study/shipping.png" class="case-study-image" />
        </figure>
        <br>
        <h4>The Presentation Stage</h4>
        <p>
          The presentation stage is the last phase of the telemetry pipeline and is, crucially, the only one where most people will interact to make important design and business decisions. This stage's purpose is to transform telemetry data into useful, measurable, and human-friendly insights regarding how an application is performing. The presentation stage is often a separate architectural component to the emission and shipping stages and is regarded as the user interface of telemetry architecture. The presentation component will pull from a given data source, format the data to be interpreted by the UI, and visualize the data in a human-friendly way.
        </p>
        <figure>
          <img src="assets/images/case-study/presentation.png" class="case-study-image case-study-image-smaller" />
        </figure>
        <br>
        <p>
          The presentation stage is often what large SaaS monitoring solutions use as their primary selling point. If the telemetry architecture doesn't implement this stage itself, the user will just do it themselves either by creating their own graphs or outsourcing to a separate UI component. Therefore, telemetry presentation is always a key component of telemetry architecture even though it does not directly impact the generation or production of telemetry data.
        </p>
        <br>
        <p>
          That’s a lot to digest! And in a way, that’s an important point for Oliver. Although he finds this interesting, he’s more interested in getting his application performance management (APM) tool up and running than understanding it deeply. How can he go about monitoring Evo T-shirts?
        </p>
        <br>
        <p>
          
        </p>

        <!-- Section 3 -->
        <h2 class="h2">3. Existing Solutions</h2> 
        <h3>3.1 SaaS</h3>
        <p>
          Oliver has several options to explore for a full application performance management tool. His need for a simple setup steers him to look at all-in-one options first. Third-party services like NewRelic and DataDog are a good place to start, as they provide out-of-the-box full observability platforms. Aside from the user copying and pasting some provided code into their application (the “emitting” or instrumentation phase), it’s all connected, set up, and ready to go. 
        </p>
        <br />
        <p>
          Hosted data storage comes with these end-to-end options. The user doesn’t even need to be aware of the kind of database being used or how it is queried by the UI; all of those details are abstracted away and the user can interact with the data exclusively through the provided UI. 
        </p>
        <br />
        <p>
          Their polished UIs include dashboards that display a range of metrics and trace sections that allow filtering and deep-dives (reminder: that’s what Oliver wants!). Many of these third-party services (including NewRelic and DataDog) go beyond metrics and traces and include both logs and events with dedicated sections, dashboards, correlations, and graphs. They often have extensive documentation to get the user up and running. 
        </p>
        <figure>
          <img src="assets/images/case-study/documentation.gif" class="case-study-image" />
          <figcaption>NewRelic Documentation</figcaption>
        </figure>
        <br />
        <p>
          As nice as this sounds, these options have immediate drawbacks for Oliver. First, he wants to retain ownership and flexible manipulation of the data. With NewRelic and DataDog, he can only interact with his data through pre-defined avenues. If he would ever leave the free NewRelic plan, he would forfeit all of his data as only paid plans can export data. Furthermore, with the free plan, NewRelic only keeps Oliver’s traces for 8 days; that will make it harder to do analysis that spans longer than a week. DataDog doesn’t offer a free plan outside of a 14-day free trial of the paid service.
        </p>
        <br />
        <p>
          Additionally, both NewRelic and DataDog are so feature-rich that their UIs are overwhelming for someone unaccustomed to observability. They have a steep learning curve that requires the user to know exactly what they want and hunt it down. Oliver doesn’t need lots of bells and whistles. He just wants to see basic requests per second, error count, latency health, and the traces behind them. Even if Evo T-shirts was willing to pay for these big services, it would be overkill for their needs. For them, the income generated would be better spent on improving the products and raising the business profile through marketing.
        </p>
        <figure>
          <img src="assets/images/case-study/overwhelming-uis.png" class="case-study-image" />
        </figure>
        <br />
        <h3>3.2 DIY</h3>
        <p>
          Because Oliver wants something simpler and custom to their needs, he’s left with a DIY via Open Source approach. He has a plethora of options for how to stitch it all together: OpenTelemetry, Jaeger, Zipkin, Prometheus, Grafana, etc. Then he has a long list of questions to research:
        </p>
        <ul>
          <li>What does each of these tools do?</li>
          <li>Where do their purposes end and begin?</li>
          <li>Can they all seamlessly work together?</li>
          <li>Should he batch export traces and metrics? Why or why not?</li>
          <li>What database should he use?</li>
          <li>Should he store metrics and traces together or separately?</li>
          <li>How should he connect metrics and traces?</li>
          <li>What pieces of information should he store about traces?</li>
          <li>And on and on...</li>
        </ul>
        <figure>
          <img src="assets/images/case-study/diy-approach.jpg" class="case-study-image" />
        </figure>
        <br />
        <p>
          This incurs a time cost that’s difficult to measure. Oliver and his team could combine well-established telemetry generating options like Prometheus (for metrics), Jaeger (for traces), and Grafana (for visualization), but this also means learning all three products (and more) and configuring them to work effectively together. 
        </p>
        <figure>
          <img src="assets/images/case-study/comparison-saas-diy.png" class="case-study-image" />
          <figcaption>Comparison of SaaS and DIY options</figcaption>
        </figure>
        <br />
        <p>
          What Oliver needs is an Open Source, all-in-one service, that allows him to retain ownership of his data and has a simple, meaningful connection between metrics and traces displayed in a simple UI. 
        </p>
        <br />
        <p>
          Lucky for him, that’s exactly what Horus is. 
        </p>
        <br />

        <!-- Section 4 -->
        <h2 class="h2">4. Introducing Horus</h2>

        <p>
          Horus is an open-source observability solution for microservices. Users can generate, store, and visualize correlated telemetry data, allowing them to see the real-time health of their application. 
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/logo-color.png" class="case-study-image case-study-image-smaller" />
        </figure>
        <p>
          Designed with the user in mind, it makes the integration, generation, and visualization of application monitoring seamless and straightforward. 
        </p>
        <br />
        <p>
          Built using OpenTelemetry, Horus encompasses industry-leading observability standards, technologies, and best practices to ensure that the generated telemetry data is always in the user’s hands and under their control.
        </p>
        <br />
        <p>
          With Horus, the user has the option to track both application metrics and traces to gain the observability coverage they need. Horus correlates the user’s application metrics with the traces that occurred in that given time period, providing full transparency over the interactions taking place across their system’s infrastructure in real-time.
        </p>
        <br />
        <p>
          Horus goes beyond just the emission of telemetry data. Through Docker, it sets up a network of Horus components on the user’s server to receive, format, store, and visualize incoming telemetry data. 
        </p>
        <br />
        <figure>
          <img src="assets/images/case-study/horus-docker.png" class="case-study-image" />
          <figcaption>Horus infrastructure in a Docker network</figcaption>
        </figure>
        <p>
          Horus removes the friction of combining components like Jaeger, Prometheus, and Grafana together to achieve observability by providing purpose-built components that reduce the potential set up time from hours or days to minutes. 
        </p>
        <br />
        <p>
          An additional key benefit of Horus is giving the user complete ownership over their data. Using a dockerized Timescale instance, there is no storage time limit or implicit cost involved in storing the user’s telemetry data, contrasting most SaaS monitoring solutions. Horus takes out the configuration and formatting headache of storing telemetry data whilst still remaining totally free and under the user’s control. The cost of storing data is completely dependent on where it’s hosted, and the user can decide what works best for them.
        </p>
        <br />
        <p>
          The docker network also contains an interactive visualization layer that automatically graphs the incoming metrics the application generates. 
        </p>
        <br>
        <p>
          There are three built-in metrics graphs that are available out of the box:
        </p>
        <ul>
          <li>Total requests per 10 seconds</li>
          <li>Total errors per 10 seconds</li>
          <li>Latency Health over 10 seconds</li>
        </ul>
        <figure>
          <img src="assets/images/case-study/gif-of-UI-1.gif" class="case-study-image" />
          <figcaption>Interacting with the Horus Metrics and Traces</figcaption>
        </figure>
        <br>
        <p>
          Each graph adjusts accordingly to the selected time window, giving the bigger picture of how the application is performing across a wider time scale. 
        </p>
        <br>
        <p>
          Horus removes the complexity and provides the user with everything they need to get going from the outset. The user can flow seamlessly between metrics and traces in a natural and intuitive way, simply by clicking on the graph and being taken directly to the traces that occurred in that time window. 
        </p>
        <br>
        <p>
          Want more information on a particular trace? Just click the trace you’re interested in and you’ll be directed to a waterfall chart where you can compare each of the spans and get any further details you require. 
        </p>
        <figure>
          <img src="assets/images/case-study/gif-of-UI-2.gif" class="case-study-image" />
          <figcaption>Interacting with the Horus Traces and Single Trace</figcaption>
        </figure>
        <br>
        <p>
          Horus provides a simple, elegant, and intuitive experience to navigate data while keeping the time cost to set up at an absolute minimum. Horus also comes with a rich set of detailed instructions and documentation to get you up and running.  
        </p>
        <br>
        <h3>4.2 Horus and Evo T-Shirts</h3>
        <p>
          Horus is the perfect solution for Oliver. He’s able to get both metric and tracing coverage without needing to learn any other technologies. Horus is well-documented and designed to be set up in just minutes, so there’s no time wasted. Oliver also gets to maintain control over the data without needing to export it to a third party. Best of all, Horus is free from the outset. There are no time or storage limitations, so Evo T-shirts can invest the time and capital back into the business. 
        </p>
        <figure>
          <img src="assets/images/case-study/evo-with-horus.gif" class="case-study-image" />
          <figcaption>Evo T-Shirts with Horus instrumentation</figcaption>
        </figure>
        <br>

        <!-- Section 5 -->
        <h2 class="h2">5. Horus Architecture</h2>
        <p>
          Horus has four distinct but interrelated components: the horus agent, connector, database, and UI. We wanted to develop an end-to-end observability tool, which means all the way from generating metrics and traces to a UI where the user can visualize them.  
        </p>
        <figure>
          <img src="assets/images/case-study/generic-full-horus.png" class="case-study-image" />
          <figcaption>The full Horus infrastrcture + Horus instrumented in a microservice application</figcaption>
        </figure>
        <br>
        <h4>5.1 Horus Agent</h4>
        <p>
          The horus agent embodies the first stage of a telemetry architecture: the emitting stage. Once installed in the application, it generates metrics and traces, collects them, and sends them to the connector. It consists of an npm package that the user installs and configures slightly. Currently, Horus is for Node applications, and part of the future work is to develop instrumentation — the horus agent — for additional languages.
        </p>
        <figure>
          <img src="assets/images/case-study/evo-with-horus.gif" class="case-study-image" />
          <figcaption>Evo T-Shirts with horus agent installed</figcaption>
        </figure>
        <br>
        <p>
          Many open-source options start directly after this stage. Their documentation points the user to OpenTelemetry documentation and has them set it up themselves. We saw this stage as imperative to automate and simplify for two main reasons. First, OpenTelemetry is difficult to use and understand for a newcomer to this sphere. As it’s still an emerging technology, the documentation for it is somewhat sparse. Thus, it is difficult to make it work without significant trial and error. 
        </p>
        <br>
        <p>
          Second, there are many types of metrics that the user could generate, and this can be difficult to figure out. Horus is made to work with three: number of requests, number of errors, and latency. These three are the most common and the most important metrics across even the SaaS platforms. By instrumenting these ahead of time, we take the guesswork out of which metrics to generate. 
        </p>
        <br>
        <p>
          Additionally, we implemented a queuing and batching system for the traces. By sending traces to the connector in this way, we allow much higher requests per second to be successfully received. Through load testing with Artillery.io, we saw how this feature of the instrumentation dramatically improved successful requests per second — by literally 1800%.
        </p>
        <br>
        <h5>OpenTelemetry</h5>
        <p>
          We chose OpenTelemetry as our instrumentation and telemetry data type primarily because we wanted a singular tool for generating both metrics and trace. Some tools only generate traces (Jaeger, Zipkin) whereas other tools only generate metrics (Prometheus). 
        </p>
        <br>
        <p>
          Another reason that prompted us to use OpenTelemetry is its inclusion of a Node SDK which provides automatic generation of traces. This abstracts away the configuration of what information is on each trace or how they are even generated, and it sufficiently covers the details we wanted.
        </p>
        <br>
        <p>
          The auto instrumentation calls a single function which automatically detects and generates telemetry data. It then sends this data to the chosen endpoint (where the connector is located, described below), avoiding the need to add extra lines of code to the application.
        </p>
        <figure>
          <img src="assets/images/case-study/opentelemetry.svg" class="case-study-imagen case-study-image-smaller" />
        </figure>
        <br>
        <p>
          The Node auto instrumentation package generates spans for a wide variety of technologies: HTTP, MySQL, Express, and more. We also chose to add MongoDB instrumentation for slightly wider coverage.
        </p>
        <br>
        <p>
          Lastly, OpenTelemetry is quickly becoming the standard in the software industry for telemetry data, so it was a good technology to learn.
        </p>
        <br>
        <p>
          The main disadvantage of OpenTelemetry is its novelty. Because it is a project rapidly growing and expanding, the documentation can sometimes be confusing or downright nonexistent. However, as mentioned before, this problem is solved from the user’s perspective thanks to the development of the npm package <code>horus-agent</code>, which extracts away the complexity.
        </p>
        <br>
        <h4>5.2 Connector</h4>
        <p>
          The horus agent sends all the data it generates to the connector. The connector then parses and transforms the data for easy storing and querying. It inserts both the metrics and traces into tables in the Timescale database for future use.
        </p>
        <figure>
          <img src="assets/images/case-study/connector.png" class="case-study-image" />
          <figcaption>Horus connector</figcaption>
        </figure>
        <br>
        <p>
          When using OpenTelemetry, it’s common to have an OpenTelemetry Collector. However, we decided to forego it completely. One of its main purposes is to ingest telemetry data from different sources (such as Prometheus and Jaeger) and then transform it into a common format. Since we used OpenTelemetry exclusively, this was unnecessary.
        </p>
        <br>
        <h4>5.3 Database</h4>
        <p>
          For a database, we chose a time-series database because it’s optimized for time-stamped data. Because time is a central part of our schema (how we chose to connect traces and metrics), having a time-focused schema and database is key. 
        </p>
        <br>
        <p>
          Time-series databases are also ideal when appending frequent and large amounts of data, but not modifying/deleting it. This is exactly what our use case is since when a metric or trace is created, there is no need to ever modify it.
        </p>
        <figure>
          <img src="assets/images/case-study/database.png" class="case-study-image" />
          <figcaption>Horus Database — replace with newer image (sans relational)</figcaption>
        </figure>
        <br>
        <p>
          We chose to use Timescale specifically, which is based on PostgreSQL, and is also one of the most popular open-source object-relational database systems. The proven maturity and high performance of Timescale and PostgreSQL made it a natural choice.
        </p>
        <br>
        <h4>5.4 UI</h4>
        <p>
          The last part of the architecture, and arguably the most important to the user, is the UI. This is the primary component that the user interacts with and is the source of all data visualization for Horus.
        </p>
        <br>
        <p>
          After pursuing several other options (see <i>Challenges and Trade-offs</i> for details), we settled on creating our own UI. This allowed us to choose our database, customize the schema, and mold the UI into the intuitive vision we had.
        </p>
        <br>
        <p>
          The UI is composed of two parts: the client and the server.
        </p>
        <figure>
          <img src="assets/images/case-study/UI.png" class="case-study-image" />
          <figcaption>Horus UI</figcaption>
        </figure>
        <br>
        <p>
          The server communicates with the TimescaleDB instance. It is an Express server that makes queries to retrieve data and then sends them on to the client in JSON format, where the data is then displayed in various graphs.
        </p>
        <br>
        <p>
          The client is a NextJS application, which is a React-based framework. It retrieves data from the server via HTTP and displays it in a way that correlates metrics and traces so the user can find what they’re after easily.
        </p>
        <br>
        <p>
          The UI has three pages: the metrics page, the traces page, and the spans page.
        </p>
        <figure>
          <img src="assets/images/case-study/ui-overview.gif" class="case-study-image" />
          <figcaption>Overview of Horus UI</figcaption>
        </figure>
        <br>
        <p>
          Initially, the user loads the metrics page, which acts as the home page. Here the user can see three dashboards with the three metrics: requests per 10 seconds, errors per 10 seconds, and latency of requests over 10 seconds.
        </p>
        <br>
        <p>
          The user can choose the timeframe of the displayed data: the last 15 minutes, last hour, last 24 hours, or last week. If they see something of interest, they can click on any of the data points in the dashboard and the UI will display to the second page: the traces page.
        </p>
        <br>
        <p>
          The traces page displays the traces that happened within a particular timestamp. This timestamp is based on which data point was clicked for the user. In the list of traces, there is some basic information about each of them. The user can click on any of the traces and they will get to the spans page.
        </p>
        <br>
        <p>
          In this page, it is possible to see all the spans that are part of a particular trace. The user can see a waterfall chart with the time that each span needed until it was completed. It is also possible to click on a particular span and see all the info that is stored about it.
        </p>
        <br>
        <!-- Section 6 -->
        <h2 class="h2">6. Challenges & Trade-offs</h2>
        <p>
          We encountered several engineering challenges and forks in the road while creating Horus. What follows are a few of the most central to Horus’s design and infrastructure.
        </p>
        <br>
        <h4>6.1 Connecting Metrics and Traces</h4>
        <p>
          A key feature of Horus is the clear link between metrics and traces. However, generating related metrics and traces is only the beginning — you have to store and query them while maintaining their relationship. How do you show the “right” traces? 
        </p>
        <br>
        <p>
          Several observability tools rely on an assigned ID to maintain this connection. When the metrics and traces are being generated, before they’re stored in the database, the trace is given a metric ID and the metric is given a trace ID. 
        </p>
        <br>
        <p>
          What happens, though, if a metric is related to multiple traces, or if a trace is related to multiple metrics? For example, a trace that contains an error span is easily included in all three of the RED metrics: it’s included in the amount of requests, the amount of errors, and the average wait time for a response. Does that trace get an array of metric IDs to choose from? Similarly, a metric data point generally covers an interval of time and does not have a one-to-one relationship with a trace. Should the metric point contain an array of all trace IDs that it covers?
        </p>
        <figure>
          <img src="assets/images/case-study/many-traces-schema.png" class="case-study-image" />
          <figcaption>A potential traces array stored across all three metric datapoints</figcaption>
        </figure>
        <br>
        <p>
          Chronosphere, an observability platform, solves this issue by only storing one “important” trace per metric data point. If the trace passes some checks (e.g. it has a status code of 500), it is chosen as the most important trace and stored. All other traces that the metric data point referenced are discarded. 
        </p>
        <figure>
          <img src="assets/images/case-study/chronosphere-architecture.png" class="case-study-image" />
          <figcaption>Chronosphere Architecture</figcaption>
        </figure>
        <br>
        <p>
          From a storage standpoint (especially with large microservices architectures), this trade-off makes sense. However, we didn’t like the idea of choosing what was important to the user. Yes, 500 server code traces are important, but what if, in less egregious-error situations, our algorithm chose the wrong “most important” trace? Doesn’t it depend on what the user is looking for? 
        </p>
        <br>
        <p>
          The flow of Horus is from metric to traces, so the metric needs to hold information to point to the related traces. An array of 700+ IDs didn’t seem like a wise approach for when the user was receiving 70 RPS (requests per second), especially when that would need to be potentially also stored across EPS (errors per second) and latency (the metrics of Horus are captured every 10 seconds).
        </p>
        <br>
        <p>
          Thus, we settled on an organic connection: time. Since metrics cover an interval of time, we only have to query traces that fall within that interval of time in order to display the “right” traces. It sounds so simple!
        </p>
        <br>
        <p>
          And it would have been... were it not for timezones.
        </p>
        <br>
        <h4>6.2 Timezones</h4>
        <p>
          Timezones brought two specific challenges: database storage and UI querying.
        </p>
        <br>
        <h5>Database Storage</h5>
        <p>
          The metrics data points we generated with OpenTelemetry did not contain a timestamp for their creation. Because we needed to connect them to traces through a timeframe, we needed to give them one. Within the connector, we added a simple JavaScript <code>Date.now()</code> and converted it to a PostgreSQL timestamp upon insertion. 
        </p>
        <br>
        <p>
          However, this Date object is subject to the time zone it detects. If the connector was being hosted on a machine with EST, all of the metrics data points had timestamps according to EST. If the connector was on a GMT machine, the timestamps all reflected GMT. This proved to be problematic because then the timezone is completely subjective — how can the UI server know what time frame to query for? Did the traces occur at 12:17 (EST) or 17:17 (GMT)?
        </p>
        <figure>
          <img src="assets/images/case-study/different-timezones.png" class="case-study-image" />
          <figcaption>At the same moment, two different times can be produced with <code>Date()</code></figcaption>
        </figure>
        <br>
        <p>
          We chose to conform to UTC for normalization and simplicity’s sake. Instead of depending on the local machine’s timezone, we added a conversion in the connector that ensures the timestamp for the metric conforms to UTC no matter what.
        </p>
        <br>
        <h5>UI Querying</h5>
        <p>
          Some tools, such as Docker and PostgreSQL, default to the UTC timezone. Other technology, however, defaults to the browser which defaults to the local machine’s timezone. Nivo graphs fall into the latter category. 
        </p>
        <br>
        <p>
          Nivo graphs are a key part of the Horus UI because these graphs are the main substance of the front page — which is all of the metrics. Although the metric data point timestamps were in UTC (in the database), the Nivo graph would convert it to the host machine’s time zone when displaying them. This worked fine for one of the creators of Horus living in England (GMT matches UTC timezone), but it did not work for any of his colleagues. 
        </p>
        <br>
        <p>
          This affected how we queried traces from the database because we used that timestamp (now often inaccurate) to find all relevant traces. For example, if the metric datapoint is in EST and it then queries the database in EST (but the database is in UTC), the query is off by 5 hours, thus displaying the wrong traces (or no traces at all, as can be seen in the diagram below). We tried different dynamic approaches to conversions that would allow the program to self-correct. Sometimes our solutions would over-correct: it seemed the program thought it was in EST when it was in GMT, so it added five hours when it shouldn’t have.
        </p>
        <figure>
          <img src="assets/images/case-study/missing-traces.gif" class="case-study-image" />
          <figcaption>Traces seem to be "missing" when there are metrics due to a timezone discrepancy</figcaption>
        </figure>
        <br>
        <p>
          We ultimately decided to allow the Nivo graph to display metrics in the timezone of the host machine, but then corrected it for trace queries. For simplicity’s sake, we chose to limit the user to only relative timeframes (e.g. the last 15 minutes, the last 24 hours, etc.). Said another way, we fixed it behind the scenes and hid the complexity from the user. We have a newfound respect for all who intimately work with timezone adaptability.
        </p>
        <br>
        <h4>6.3 Visualization Trade-offs</h4>
        <p>
          Our initial prototype used a visualization layer provided by Grafana, which is an open-source dashboard monitoring solution. 
        </p>
        <br>
        <p>
          Grafana is one of the leading service providers for querying, visualizing, and interpreting telemetry data. It has the ability to receive and translate metrics, traces, and logs from a number of different storage sources and display them in a way that a user can easily gain insights and make decisions about their infrastructure. 
        </p>
        <br>
        <p>
          In essence, Grafana provided the perfect solution to integrate the presentation stage into our application. It could visualize, and graph, both metrics and traces and had the ability to query our database of choice, TimescaleDB, using the built-in SQL querying utility. 
        </p>
        <br>
        <p>
          Unfortunately, as we integrated Grafana into our own application infrastructure, it became apparent that there were limitations. While Grafana does offer a range of storage solutions, only some types of telemetry data can be queried from each database. In our case, we were able to query TimescaleDB for metrics, but we were unable to retrieve trace data that could be interpreted by Grafana. Grafana could only visualize trace data received from proprietary tracing solutions like Jaeger, Zipkin, X-Ray, or Tempo. 
        </p>
        <br>
        <p>
          This left us with two potential options to explore. 
        </p>
        <figure>
          <img src="assets/images/case-study/alternate-full.png" class="case-study-image" />
          <figcaption>Two options of architecture: with Grafana or with a custom UI</figcaption>
        </figure>
        <br>
        <p>
          First, we could commit to using Grafana as our visualization layer and generate traces using a pre-existing service like Jaeger or Zipkin. This was already an unattractive option because we wanted to use OpenTelemetry as our only mechanism for emitting telemetry data. Introducing more technologies meant introducing more dependencies and thereby adding greater complexity to our system. 
        </p>
        <br>
        <p>
          Moreover, we only wanted a single database to store both traces and metrics to keep our data tightly coupled together. Including Jaeger or Zipkin would mean that we would need another database (both of these technologies were only compatible with Cassandra and Elasticsearch databases). We knew that by adding another database, the complexity to our system architecture would increase. Furthermore, it would open us up to more points of failure and potentially create time synchronicity issues. 
        </p>
        <figure>
          <img src="assets/images/case-study/alternate-grafana.png" class="case-study-image" />
          <figcaption>Alternative architecture that includes Grafana</figcaption>
        </figure>
        <br>
        <p>
          Although this route clearly held difficulties along the way, we knew that the technologies were compatible, and that gave us confidence. It would just be a matter of configuring them to operate with one another. However, this would come at a cost. We would lose the simplicity and elegance of our initial architecture. 
        </p>
        <br>
        <p>
          Our second option was more radical: we could build our own user interface. 
        </p>
        <figure>
          <img src="assets/images/case-study/alternate-horus.png" class="case-study-image" />
          <figcaption>The architecture we chose that includes a custom UI</figcaption>
        </figure>
        <br>
        <p>
          Going down this route had several advantages. First, we could be faithful to our system’s architecture. By configuring our connector component to receive data, format it, and store it in a single database, we would be able to remove several unnecessary components from our infrastructure. This would reduce the overall complexity and mitigate points of failure. We would rely on the frontend’s querying service to retrieve the data it needed from the database, format it to be interpreted by our graphical and visual components, and then pass it to our frontend to be rendered on our application interface. 
        </p>
        <br>
        <p>
          Secondly, we would have complete control over how our user interface both operated and looked. This gave us the option to only include the key features we wanted to provide to the user, making the application a lot less visually intimidating to the type of user stated in our use case. 
        </p>
        <br>
        <p>
          This route also had potential drawbacks. The upfront engineering effort would be much greater than working with preexisting components; we would need to implement the UI using a React Framework with a backend querying service to do the heavy lifting. Moreover, we would need to gain fluency in specific visualization tools such as graphing and chart packages to make the data more human-friendly.  
        </p>
        <br>
        <p>
          Ultimately, the decision came down to adding complexity and working with preexisting components or keeping complexity to a minimum and investing the time and energy to produce an interactive interface. 
        </p>
        <br>
        <p>
          After discussing and evaluating our options, we came to the conclusion that keeping the architecture as straightforward and lightweight as possible was essential to meeting our use case. Moreover, the autonomy and control gained by producing our own user interface was in itself worth the engineering effort and energy. We would have greater say in how our user would interact with our application and again further fulfill the requirements of the user in our use case. In hindsight, we are grateful for that decision and it was worth it.
        </p>
        <figure>
          <img src="assets/images/case-study/ui-overview.gif" class="case-study-image" />
          <figcaption>Overview of Horus UI</figcaption>
        </figure>
        <br>
        <!-- Section 6 -->
        <h2 class="h2">7. Horus Installation & Set Up</h2>
        <p>
          There are two steps to setting up Horus:
        </p>
        <ul>
          <li>Install and set up <code>horus-agent</code> in the root service of the application</li>
          <li>Deploy the Horus infrastructure via Docker</li>
        </ul>
        <br>
        <h4>7.1 Setting Up Instrumentation</h4>
        <p>
          First, install the <code>horus-agent</code> with <code>npm</code>. This should be installed in the backend/server-side of the root service as well as each dependent service.
        </p>
        <figure>
          <img src="assets/images/case-study/npm-install-horus-agent.gif" class="case-study-image" />
        </figure>
        <br>
        <p>
          Then integrate it into your routes according to the <a href="www.npmjs.com/package/horus-agent">npm README</a>. Note that the MetricsAgent is only needed in the root service, whereas the TracingAgent should be placed in each dependent service so as to capture every single span.
        </p>
        <br>
        <p>
          Second, update the <code>config.json</code> file to point to the endpoint of choice. If hosting Horus on a local machine, the endpoint can remain as is with <code>localhost</code>. If deploying to a different server (e.g. a DigitalOcean Droplet, an AWS EC2 instance, etc.), change the endpoint to point to that IP address or domain name. 
        </p>
        <br>
        <p>
          The <code>config.json</code> file can be found within <code>node_modules/horus-agent</code>.
        </p>
        <figure>
          <img src="assets/images/case-study/my-domain-name.gif" class="case-study-image" />
        </figure>
        <br>
        <h4>7.2 Deploying with Docker</h4>
        <p>
          Download Docker and/or Docker Compose if not already on the machine. Clone the <a href="github.com/try-horus/horus-compose">horus-compose</a> repo. In the terminal and in the same folder as the <code>docker-compose.yaml</code> file, run <code>docker-compose up</code>. 
        </p>
        <figure>
          <img src="assets/images/case-study/docker-compose-simple.gif" class="case-study-image" />
        </figure>
        <p>
          This deploys the full Horus infrastructure within a Docker network.
        </p>
        <br>
        <p>
          Voíla! Horus is set up. 
        </p>
        <br>
        <!-- Section 7 -->
        <h2 class="h2">8. Future Work</h2>
        <p>
          And that’s Horus! Although it fulfills all of Oliver’s needs, an open-source project is never fully finished. These are some of the features that we would like to pursue in future iterations:
        </p>
        <ul>
          <li>Developing more pages in the UI: e.g. a page where the user can choose a custom timeframe and view all the traces that happened in that timeframe</li>
          <li>Support more languages different in addition to JS for the generation of telemetry data</li>
          <li>Investigate the hardware and system requirements for hosting Horus for larger microservices</li>
        </ul>
      
        
        <!-- Section 8 -->
        <h2>9. Team</h2>
        <br>
        <br>
        <div class="section team-section">
          <div class="container">
            <div data-duration-in="300" data-duration-out="100" class="tabs w-tabs">
              <div data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a" style="
                    transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1)
                      rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg);
                    transform-style: preserve-3d;
                    opacity: 0;
                  " class="tabs-content w-tab-content">
                <div>
                  <div class="team-grid">
                    <div class="team-member-wrap">
                      <img src="assets/images/team/Rich-modified.png" loading="lazy" alt="" />
                      <div class="team-member-info">
                        <div class="team-member-name">Richard Morris</div>
                        <div class="team-member-location">London, UK</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:richwynmorris@protonmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.richmorris.co.uk" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <!-- Add linked in for RM-->
                          <a href="" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/Callie-modified.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Callie Buruchara</div>
                        <div class="team-member-location">Maryland, USA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <!-- Add CB email-->
                          <a href="mailto:callie.buruchara@hey.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://callieb.dev/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <!-- Add CB linkedin-->
                          <a href="" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/José-modified.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">José de la Puente</div>
                        <div class="team-member-location">London, UK</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:14jdelap@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.josedelapuente.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/ja-puente/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/juan-modified.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Juan García</div>
                        <div class="team-member-location">Madrid, Spain</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:srgarci79@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <!-- Add website for JG-->
                          <a href="" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/juan-garc%C3%ADa-moreno-21bbbb155/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
    </article>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
      type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
      crossorigin="anonymous"></script>
    <script src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
      type="text/javascript"></script>
    <script>
      /*!
       * toc - jQuery Table of Contents Plugin
       * v0.3.2
       * http://projects.jga.me/toc/
       * copyright Greg Allen 2014
       * MIT License
      */
      !function (a) { a.fn.smoothScroller = function (b) { b = a.extend({}, a.fn.smoothScroller.defaults, b); var c = a(this); return a(b.scrollEl).animate({ scrollTop: c.offset().top - a(b.scrollEl).offset().top - b.offset }, b.speed, b.ease, function () { var a = c.attr("id"); a.length && (history.pushState ? history.pushState(null, null, "#" + a) : document.location.hash = a), c.trigger("smoothScrollerComplete") }), this }, a.fn.smoothScroller.defaults = { speed: 400, ease: "swing", scrollEl: "body,html", offset: 0 }, a("body").on("click", "[data-smoothscroller]", function (b) { b.preventDefault(); var c = a(this).attr("href"); 0 === c.indexOf("#") && a(c).smoothScroller() }) }(jQuery), function (a) { var b = {}; a.fn.toc = function (b) { var c, d = this, e = a.extend({}, jQuery.fn.toc.defaults, b), f = a(e.container), g = a(e.selectors, f), h = [], i = e.activeClass, j = function (b, c) { if (e.smoothScrolling && "function" == typeof e.smoothScrolling) { b.preventDefault(); var f = a(b.target).attr("href"); e.smoothScrolling(f, e, c) } a("li", d).removeClass(i), a(b.target).parent().addClass(i) }, k = function () { c && clearTimeout(c), c = setTimeout(function () { for (var b, c = a(window).scrollTop(), f = Number.MAX_VALUE, g = 0, j = 0, k = h.length; k > j; j++) { var l = Math.abs(h[j] - c); f > l && (g = j, f = l) } a("li", d).removeClass(i), b = a("li:eq(" + g + ")", d).addClass(i), e.onHighlight(b) }, 50) }; return e.highlightOnScroll && (a(window).bind("scroll", k), k()), this.each(function () { var b = a(this), c = a(e.listType); g.each(function (d, f) { var g = a(f); h.push(g.offset().top - e.highlightOffset); var i = e.anchorName(d, f, e.prefix); if (f.id !== i) { a("<span/>").attr("id", i).insertBefore(g) } var l = a("<a/>").text(e.headerText(d, f, g)).attr("href", "#" + i).bind("click", function (c) { a(window).unbind("scroll", k), j(c, function () { a(window).bind("scroll", k) }), b.trigger("selected", a(this).attr("href")) }), m = a("<li/>").addClass(e.itemClass(d, f, g, e.prefix)).append(l); c.append(m) }), b.html(c) }) }, jQuery.fn.toc.defaults = { container: "body", listType: "<ul/>", selectors: "h1,h2,h3", smoothScrolling: function (b, c, d) { a(b).smoothScroller({ offset: c.scrollToOffset }).on("smoothScrollerComplete", function () { d() }) }, scrollToOffset: 0, prefix: "toc", activeClass: "toc-active", onHighlight: function () { }, highlightOnScroll: !0, highlightOffset: 100, anchorName: function (c, d, e) { if (d.id.length) return d.id; var f = a(d).text().replace(/[^a-z0-9]/gi, " ").replace(/\s+/g, "-").toLowerCase(); if (b[f]) { for (var g = 2; b[f + g];)g++; f = f + "-" + g } return b[f] = !0, e + "-" + f }, headerText: function (a, b, c) { return c.text() }, itemClass: function (a, b, c, d) { return d + "-" + c[0].tagName.toLowerCase() } } }(jQuery);
    </script>
    <script>
      /* initialize */
      $('.toc').toc({
        'selectors': 'h2', //elements to use as headings
        'container': 'article', //element to find all selectors in
        'smoothScrolling': true, //enable or disable smooth scrolling on click
        'prefix': 'toc', //prefix for anchor tags and class names
        'highlightOnScroll': true, //add class to heading that is currently in focus
        'highlightOffset': 100, //offset to trigger the next headline
      });
    </script>
</body>

</html>